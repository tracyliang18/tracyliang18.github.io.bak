#语音识别简介
来苏州aispeech公司实习已有一个月多了，前段时间的工作与语音识别相关，分享些学到的东西，如有表述错误的地方欢迎指出。

##历史
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;语音识别的研究工作大约开始于20世纪50年代，当时AT&T Bell实验室基于共振峰提取技术实现了第一个可识别十个英文数字的语音识别系统——Audry系统。

>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;60年代，计算机的应用推动了语音识别的发展。这时期的重要成果是提出了动态时间规划（DP）和线性预测分析技术（LPC），其中后者较好地解决了语音信号产生模型的问题，对语音识别的发展产生了深远影响。

>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;70年代，语音识别领域取得了较大进展。在理论上，LP技术得到进一步发展，动态时间归正技术（DTW）基本成熟，特别是提出了矢量量化（VQ）和隐马尔可夫模型（HMM）理论。在实践上，实现了基于线性预测倒谱和DTW技术的特定人孤立语音识别系统。

>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80年代，MFCC的参数提取技术和HMM模型的深入使用使得语音识别技术得到进一步的发展，语音识别的问题逐步在理论体系上得到了比较完整和准确的描述，同时在实践上又逐步研发出效率较高的解决算法。

>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;90年代以来，在美国国防部的Darpa测试、Ears计划、近期的Gales计划，以及我国863计划等推动下，一大批高水平的研究机构和企业加入到 语音识别的研究领域，极大地推动了语音识别技术的发展和应用。语音识别系统已经从过去的小词汇量、孤立词识别、特定人识别、安静环境等简单任务逐步发展到 大词汇量、连续语音、非特定人、噪声环境下的识别任务，从单纯的语音识别任务发展到语音翻译任务，从实验室系统走向商用系统。

>>>——历史摘自国内语音识别领军“科大讯飞“主页


##初探
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;语音识别就是一个语音到文字（speech to text )的过程，也就是一个解码的过程。最简单的想法就是找到一个伟大的神秘函数Text = f(voice),输入语音，然后输出文字。基本上属于不可能任务。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;语音和文字，哪个在先？从历史来看显然是先有语音后有文字的，从人类学习的过程也是先语音后文字的（想一想婴儿就好理解了）。基本上所有动物都能通过声音交互信息，人类也是如此，而人类多了一个符号化的东西——文字，去表达，交流，记载。语音是自然的产物，而文字是智慧的产物。所有文字符号在一定语义下都必然对应这一个语音，而语音却未必能找到对应的文字符号。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为什么我们听到语音就能得出相应的句子呢（或者几个）？原因是我们在读书识字的时候在大脑建立了音-字这种联系，而且直观上看这种联系是很直接的。当然音-字联系在大脑中还借助了第三方“语义”来加强这种关系。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上面可以看出，人在识别语音到文字的过程具备两个条件：1.知识（音，文字符号） 2.音到文字的决策算法。
	
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说了这么多，没有一点形式化的东西，这样的话问题是无法交给计算机去解决的。好了入正题了。
	

###单字识别
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面我们讲述单字识别的整个流程。假定输入的语音都是单字的发音。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;语音识别成文字，其实就是寻找这样一个字w，给定一个语音o，使得p(w|o)最大。等价为p（w）p（o|w）。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了寻找“语音”到“文字”的关系，我们采用两头逼近中间的方法试图建立这种关系。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们先对语音做一些处理。计算机拿到的是数字语音信号，但单从一堆时域信号很难看出个什么眉目，而且数据量大，难以处理。我们希望对拿到的信号进行一些处理，达到用较少的数据去尽可能多地表达语音的信息，分帧处理和频谱变换才是王道。我们以10ms为一帧处理，对这一段信号进行进行倒频谱计算或LPC编码而最终达到用若干个系数去表达这个音频信号，这若干个系数组合到一起就是这段信号的特征向量了。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特征向量应该符合以下几条准则：1.计算方便,运算量少 2.维数在能表现特征前提下尽量少 3.特征向量能表达通用特征。对于语音识别，所选择的特征向量不能偏袒个别音节。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，我们实现了对语音信号的预处理，我们对文字符号也做些预处理。想让文字向语音靠拢，最自然的当然是拼音方案，当然也可以用国际音标方案（用什么方案没关系，你也可以自己发明一套语音表示系统，怎样表示语音不是重点，重点是语音本身）。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以江字为例，首先表示为拼音：江 ------> jiang1。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再进一步，拆分为音节：江 ------> j i ang1。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;语音因为有连读发声的特点，一些人又提出了tri-phone model 去表达文字。就是将表达二中的每一个音节，携带上左右两个”好基友“音节，左边用“-”符号连接，右边用“+“符号连接。如下

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;江 ------> j+i j-i+ang1 i-ang1

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在音节下面还有更小的语音单元——音素（phoneme），在实际识别时也是用音素去建模的，但音素概念比较抽象，在本文没必要深入到这些细节，有兴趣的可以再深入了解下。上面例子中“江”最终会表达成 t-j+ia1 j-ia1+ng ie1-ng。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们从语音数据每一帧数据得到一个特征向量，从文字符号得到了另一种符号——tri-phone符号。现在我们要构建它们的联系。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;构建音节符号到语音特征向量的关系是基于统计和随机过程。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先理解统计，以声母y为例，理论上说声母y可以发出无穷个音，但不同音发出的概率是有差别的，个别音发出的概率可能很大，个别音发出的概率会很少，甚至为零。因此，每一个音节有一个语音的概率分布，或者说每一个音节有一个产生语音特征向量的概率分布（下文中语音等价于特征向量）。这个概率分布可以简单地理解为学术上说的声学模型。音节产生某一确定向量的概率为零，对我们有用的是音节产生某类向量的概率。现在面临两个问题，一是怎么得到这些概率，二是怎样让向量分类。对每个音节需要大量的语音数据来训练出这样一个声学模型，而向量分类则可以用聚类算法去解决。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接着我们理解随机过程。我们可以将发音理解成一个马尔可夫过程（转移概率仅与当前相关）。发音其实就是音节的状态转移的过程。一个音节有一定的概率转移到下一个音节，也有一定的概率停留在本音节，也有一定的概率转移到下下个音节，后两种情况可以理解为现实中的连读和跳读。如下图(缺图）:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，对应于每一个字我们都可以得到如上图的模型。可是这些转移概率怎么得到呢？含有未知参数的马尔可夫模型就是俗称的隐马尔可夫模型（Hidden Markov Model），得到这些未知参数涉及到另一个训练过程，有兴趣的可以了解一下HTK（HMM Tool Kit）。HMM的参数训练主要涉及到到前后向算法（又称 Baum-Welch算法）。目前HMM的参数训练只能做到局部最优的。训练的需要同一个字的大量语音数据，然后使用人工或算法的方式对音频进行强制对齐。训练的过程我还不是很明白，这个以后可以多做交流。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到这里我们已经可以做最简单的单字识别了。每一个字都具备了一个HMM模型，字里面每一个音节都具备了一个声学模型（产生特征向量概率分布），给定一串语音，我们可以计算每一个字产生这个语音序列的概率，最后概率最大的字就是我们的识别结果。
###HMM练习
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在讲解上述计算概率过程之前，先来点HMM的练习。
	
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一个所有参数都已知的HMM模型，是一个气压转移模型。每天的气压只有三种状态，高气压，中气压，低气压。不同气压伴随的天气情况会有差别。例如低气压时较容易下雨，而高气压时更有可能是晴天。在每个气压状态的旁边有各自产生不同天气的概率，如中气压状态下雨天、多云和晴天的概率分别是0.3、0.4和0.3.

!["天气转移"]("https://github.com/tracyliang18/tracyliang18.github.io/image/Weather_HMM.png“)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算：

* 若已知六天的气压状态为{高，  中，    中，  低，  低，  中}，求产生天气序列为{晴天，晴天，多云，雨，多云，晴天}的概率。

* 若已知天气序列为{晴天，晴天，多云，雨，多云，晴天}，求最有可能产生该天气序列的气压转移序列。

* 求产生天气序列为{晴天，晴天，多云，雨，多云，晴天}的概率。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;题解：

* 问题一：

>直接概率相乘，得到结果。0.8 ·0.3 · 0.4 · 0.6 · 0.3 · 0.3=5.2x10-3

* 问题二：

>有了问题一的基础，最暴力的方法就是穷举所有状态转移序列（3^6)，计算每一个序列计算出产生该天气的概率，概率最大的就是最可能的状态转移序列。问题是这样的计算复杂度太高,为N^T。vitebi search算法可以解决这个问题，该算法是一个动态规划算法，复杂度为T*N^2.(T为时间，N为状态个数）

* 问题三：

> 同样在问题一的基础，我们可以计算所有状态转移序列产生该天气的概率，然后累加的结果就是产生该天气的总概率。暴力方法的复杂度同样不能接受，前向算法或后向算法可以解决这个问题，这两种算法和vitebi search 很类似，复杂度为2*T*N^2.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;怎样计算每一个字产生给定语音的得分？每个字的得分就是每一个字产生给定语音的最大概率。把每一个字中的音节状态类比成上面例子的气压状态，把一连串语音特征向量序列（o1,o2...on)类比成上述的天气转移序列，声学模型类比成产生天气的概率分布。那么计算每一个字产生给定语音的最大概率就相当与上述问题中的问题二。

###连续词识别

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在有了单字识别基础，过渡到连续词/句的识别就不难了。同样地，我们可以为每个词训练出一个HMM模型。若干个词连接起来构建成一个句子的模型。而词和词之间的转移会有一个权重，这里需要融入语言模型（language model）的知识。语言模型简单来说就是在给定某个词的情况下，出现另一个词的概率p(w|w1)。例如我们已经识别出了“中国”，接下来“共产党”和“供禅挡”在同样声学得分的情况下，“共产党”在语言模型上的得分要更高，因此会选择“共产党”。当然这里举的例子不是很恰当，因为 “供禅挡”不是一个词，而且这里仅仅考虑到局部最优。语言模型按阶数n称为n-gram模型。上述的语言模型是一阶的。像p（w|w1,w2)就是二阶的。语音识别中常用的语言模型是二阶或者三阶的。公司最近也在对语言模型的阶数做实验来提高识别率。
	
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果对所有句子序列都做一个计算从而得出最有可能的句子，计算量是很十分惊人的。于是我们会对识别的句子上做一些限制，而且在搜索算法和搜索网络上要做优化。像我目前做的一个在手机上识别的模块，只能识别符合特定语法的句子，而且字典里的词较少，所以搜索速度较快，识别可以直接在手机本地上跑，而不用利用服务器的资源。

#结语
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前对语音识别的了解也没有很深入，很多东西也只能谈个大概，没有深入到技术细节。当中错误和不当的地方应该不少，望指正。
